{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/Panizzutti/GPT_from_scratch/blob/83c554974ce0c1904d70d8133a753f86a66611d2/divina_commedia.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OI4pMLxooyDs",
        "outputId": "8539a2c9-d3eb-496f-db7a-56abcb73f5ff"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-02-16 10:41:15--  https://github.com/Panizzutti/GPT_from_scratch/blob/83c554974ce0c1904d70d8133a753f86a66611d2/divina_commedia.txt\n",
            "Resolving github.com (github.com)... 140.82.112.3\n",
            "Connecting to github.com (github.com)|140.82.112.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: ‘divina_commedia.txt.1’\n",
            "\n",
            "divina_commedia.txt     [ <=>                ]   1.19M  --.-KB/s    in 0.09s   \n",
            "\n",
            "2025-02-16 10:41:16 (13.8 MB/s) - ‘divina_commedia.txt.1’ saved [1248086]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "0yoM_C2xoZPQ"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "CHojd65PoZPQ"
      },
      "outputs": [],
      "source": [
        "with open('divina_commedia.txt', 'r', encoding = 'utf-8') as f:\n",
        "    text = f.read()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qmzHjKvjoZPQ",
        "outputId": "de12f675-b30d-40e8-d7c2-0ba29407cb0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "<!DOCTYPE html>\n",
            "<html\n",
            "  lang=\"en\"\n",
            "  \n",
            "  data-color-mode=\"auto\" data-light-theme=\"light\" data-dark-theme=\"dark\"\n",
            "  data-a11y-animated-images=\"system\" data-a11y-link-underlines=\"true\"\n",
            "  \n",
            "  >\n",
            "\n",
            "\n",
            "\n",
            "  <h\n"
          ]
        }
      ],
      "source": [
        "print(text[:200])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Zh-yVc0oZPQ",
        "outputId": "d662c38e-7f12-4fd6-a0c4-cb1395143546"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1241594"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "len(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kb0m9Z0QoZPR",
        "outputId": "5571d7e7-b4e9-4c3d-9df8-ec67c158ded5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !\"#%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]_`abcdefghijklmnopqrstuvwxyz{}~ ·àèéìïòóù’…\n",
            "105\n"
          ]
        }
      ],
      "source": [
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(\"\".join(chars))\n",
        "print(vocab_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "mEpIirazoZPR"
      },
      "outputs": [],
      "source": [
        "char_to_int = { ch: i for i,ch in enumerate(chars)}\n",
        "int_to_char = { i: ch for i,ch in enumerate(chars)}\n",
        "encode = lambda s: [char_to_int[c] for c in s] #encoder function\n",
        "decode = lambda l: \"\".join(int_to_char[i] for i in l) #decoder function\n",
        "\n",
        "#not byte pair encoding, for simplicity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "QYC9rT7XoZPR",
        "outputId": "63edde61-cae9-4a29-9d9d-60ab6ace4e3a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cianoxxj'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "decode(encode(\"cianoxxj\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jW8AJNwCoZPR",
        "outputId": "5641df0e-e311-473c-df69-da11bcbfc168"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[66, 72, 64, 78, 1, 76, 78, 77, 67, 78]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "encode(\"ciao mondo\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "yqELeRRCoZPR"
      },
      "outputs": [],
      "source": [
        "data = torch.tensor(encode(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "EkaneMjCoZPR"
      },
      "outputs": [],
      "source": [
        "n = int(0.9 * len(data))\n",
        "train_data= data[:n]\n",
        "val_data= data[n:]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ziigIi8doZPR",
        "outputId": "81175d3d-c3c2-4246-aa0d-99dccbd61b49"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0,  0,  0,  0,  0,  0, 28,  2, 36, 47, 35])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "batch_size = 4\n",
        "block_size= 10\n",
        "\n",
        "train_data[:block_size+1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SllBaAyNoZPR",
        "outputId": "b65a4d5f-9d8f-485e-9421-89ecd2bfb55d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs:\n",
            "torch.Size([4, 10])\n",
            "tensor([[60,  3, 14, 60, 81,  3, 12,  3,  1,  1],\n",
            "        [20, 13, 14, 18, 20, 19, 13, 17, 14, 24],\n",
            "        [15, 48, 64, 77, 72, 89, 89, 84, 83, 83],\n",
            "        [12,  3, 80, 84, 72, 85, 72,  1, 75, 84]])\n",
            "targets:\n",
            "torch.Size([4, 10])\n",
            "tensor([[ 3, 14, 60, 81,  3, 12,  3,  1,  1, 37],\n",
            "        [13, 14, 18, 20, 19, 13, 17, 14, 24, 20],\n",
            "        [48, 64, 77, 72, 89, 89, 84, 83, 83, 72],\n",
            "        [ 3, 80, 84, 72, 85, 72,  1, 75, 84, 76]])\n",
            "----\n",
            "when input is [60] the target: 3\n",
            "when input is [60, 3] the target: 14\n",
            "when input is [60, 3, 14] the target: 60\n",
            "when input is [60, 3, 14, 60] the target: 81\n",
            "when input is [60, 3, 14, 60, 81] the target: 3\n",
            "when input is [60, 3, 14, 60, 81, 3] the target: 12\n",
            "when input is [60, 3, 14, 60, 81, 3, 12] the target: 3\n",
            "when input is [60, 3, 14, 60, 81, 3, 12, 3] the target: 1\n",
            "when input is [60, 3, 14, 60, 81, 3, 12, 3, 1] the target: 1\n",
            "when input is [60, 3, 14, 60, 81, 3, 12, 3, 1, 1] the target: 37\n",
            "when input is [20] the target: 13\n",
            "when input is [20, 13] the target: 14\n",
            "when input is [20, 13, 14] the target: 18\n",
            "when input is [20, 13, 14, 18] the target: 20\n",
            "when input is [20, 13, 14, 18, 20] the target: 19\n",
            "when input is [20, 13, 14, 18, 20, 19] the target: 13\n",
            "when input is [20, 13, 14, 18, 20, 19, 13] the target: 17\n",
            "when input is [20, 13, 14, 18, 20, 19, 13, 17] the target: 14\n",
            "when input is [20, 13, 14, 18, 20, 19, 13, 17, 14] the target: 24\n",
            "when input is [20, 13, 14, 18, 20, 19, 13, 17, 14, 24] the target: 20\n",
            "when input is [15] the target: 48\n",
            "when input is [15, 48] the target: 64\n",
            "when input is [15, 48, 64] the target: 77\n",
            "when input is [15, 48, 64, 77] the target: 72\n",
            "when input is [15, 48, 64, 77, 72] the target: 89\n",
            "when input is [15, 48, 64, 77, 72, 89] the target: 89\n",
            "when input is [15, 48, 64, 77, 72, 89, 89] the target: 84\n",
            "when input is [15, 48, 64, 77, 72, 89, 89, 84] the target: 83\n",
            "when input is [15, 48, 64, 77, 72, 89, 89, 84, 83] the target: 83\n",
            "when input is [15, 48, 64, 77, 72, 89, 89, 84, 83, 83] the target: 72\n",
            "when input is [12] the target: 3\n",
            "when input is [12, 3] the target: 80\n",
            "when input is [12, 3, 80] the target: 84\n",
            "when input is [12, 3, 80, 84] the target: 72\n",
            "when input is [12, 3, 80, 84, 72] the target: 85\n",
            "when input is [12, 3, 80, 84, 72, 85] the target: 72\n",
            "when input is [12, 3, 80, 84, 72, 85, 72] the target: 1\n",
            "when input is [12, 3, 80, 84, 72, 85, 72, 1] the target: 75\n",
            "when input is [12, 3, 80, 84, 72, 85, 72, 1, 75] the target: 84\n",
            "when input is [12, 3, 80, 84, 72, 85, 72, 1, 75, 84] the target: 76\n"
          ]
        }
      ],
      "source": [
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    return x, y\n",
        "\n",
        "xb, yb = get_batch('train')\n",
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('targets:')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "print('----')\n",
        "\n",
        "for b in range(batch_size): # batch dimension\n",
        "    for t in range(block_size): # time dimension\n",
        "        context = xb[b, :t+1]\n",
        "        target = yb[b,t]\n",
        "        print(f\"when input is {context.tolist()} the target: {target}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UeRmGcHQoZPR",
        "outputId": "0795303e-6747-4047-c526-a8be94c460f1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BigramLanguageModel(\n",
              "  (token_embedding_table): Embedding(105, 105)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(42)\n",
        "\n",
        "class BigramLanguageModel(nn.Module): #this simply calculate the probability of next token only on the previous token\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C) #reshape\n",
        "            targets = targets.view(B*T) #reshape in a single vector\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "m = BigramLanguageModel(vocab_size)\n",
        "m"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "logits, loss = m(xb, yb)\n",
        "print(logits.shape)\n",
        "print(loss)\n",
        "\n",
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y5_UWYdPqkOL",
        "outputId": "30012267-2e17-4a99-929b-a230a1ec9762"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([40, 105])\n",
            "tensor(4.9411, grad_fn=<NllLossBackward0>)\n",
            "\n",
            "\n",
            "CWScòAUnv/A@~>XLà>#XP-i-`d![U69\n",
            "7l,,Ye=ì]yJì[Q& JdG6o~ï%szù·#XaOF3ok…ogxX +[\"NE,>>}A;’H-b\n",
            " 7àV'T&Y5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n",
        "\n",
        "for steps in range(20000): # increase number of steps for good results...\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = m(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(loss.item())"
      ],
      "metadata": {
        "id": "81ESLYXBHNWC",
        "outputId": "15337620-2e3f-49f6-f18f-9a42144b8d2e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.584106922149658\n",
            "CPU times: user 22.6 s, sys: 641 ms, total: 23.3 s\n",
            "Wall time: 24.4 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=400)[0].tolist()))"
      ],
      "metadata": {
        "id": "ivJyD7T4H2PN",
        "outputId": "5e0dabf5-80eb-4914-bc54-10b01e8487a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " checo s-ce.25v></di l  t\",\" stria-ch'mela,\\rct_s=\" ssta girale-to int-r\">1  sfis-21.25fser\",[],[],\" no liv>\n",
            " raretret-le v>\n",
            " qure so   iapià er  re\\\"l sin sereneri\\\"diopile on pxt'allo  G13.5Va puta\\ris=\" spr 149\",\\\",[],\"per 06 e-be:{mba, .28\",\"Ne e la2<delonga 'a-31\",\" codacurenfutalipaldi;\\rattofronttiar\" follch foui clefe mocolinasì chto,\" bylline-cosea  5lo.5forosì Gla pre-che .6\"cheso\\r\"\\r\",\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}