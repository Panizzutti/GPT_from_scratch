{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Panizzutti/GPT_from_scratch.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OI4pMLxooyDs",
        "outputId": "761bef98-6cb3-45a4-9bfa-f0dd9a442769"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'GPT_from_scratch' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "0yoM_C2xoZPQ"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "CHojd65PoZPQ"
      },
      "outputs": [],
      "source": [
        "with open('divina_commedia.txt', 'r', encoding = 'utf-8') as f:\n",
        "    text = f.read()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qmzHjKvjoZPQ",
        "outputId": "8076096b-c5ff-4d64-d2b2-6af207d2bcf9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LA DIVINA COMMEDIA\n",
            "di Dante Alighieri\n",
            "INFERNO\n",
            "\n",
            "\n",
            "\n",
            "Inferno: Canto I\n",
            "\n",
            "  Nel mezzo del cammin di nostra vita\n",
            "mi ritrovai per una selva oscura\n",
            "ché la diritta via era smarrita.\n",
            "  Ahi quanto a dir qual era è\n"
          ]
        }
      ],
      "source": [
        "print(text[:200])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Zh-yVc0oZPQ",
        "outputId": "688250a3-c74e-475a-bb2c-9919d39c6502"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "537093"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "len(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kb0m9Z0QoZPR",
        "outputId": "9134b1c4-1a16-46b2-bb89-e7a4c4f2cc80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !\"'(),-.:;?ABCDEFGHILMNOPQRSTUVXZabcdefghijlmnopqrstuvxyz~àèéìïòóù\n",
            "68\n"
          ]
        }
      ],
      "source": [
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(\"\".join(chars))\n",
        "print(vocab_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "mEpIirazoZPR"
      },
      "outputs": [],
      "source": [
        "char_to_int = { ch: i for i,ch in enumerate(chars)}\n",
        "int_to_char = { i: ch for i,ch in enumerate(chars)}\n",
        "encode = lambda s: [char_to_int[c] for c in s] #encoder function\n",
        "decode = lambda l: \"\".join(int_to_char[i] for i in l) #decoder function\n",
        "\n",
        "#not byte pair encoding, for simplicity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "QYC9rT7XoZPR",
        "outputId": "dd5f0a7c-47f2-4169-c1fd-7ced9cafff2e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cianoxxj'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "decode(encode(\"cianoxxj\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jW8AJNwCoZPR",
        "outputId": "92c85b8c-4a53-4d2b-cc56-6d9af2d00e83"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[37, 43, 35, 48, 1, 46, 48, 47, 38, 48]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "encode(\"ciao mondo\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "yqELeRRCoZPR"
      },
      "outputs": [],
      "source": [
        "data = torch.tensor(encode(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "EkaneMjCoZPR"
      },
      "outputs": [],
      "source": [
        "n = int(0.9 * len(data))\n",
        "train_data= data[:n]\n",
        "val_data= data[n:]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ziigIi8doZPR",
        "outputId": "2137ae43-46f5-4983-c1a1-97a90ebab594"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([22, 13,  1, 16, 21, 32, 21, 24, 13,  1, 15])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "batch_size = 4\n",
        "block_size= 10\n",
        "\n",
        "train_data[:block_size+1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SllBaAyNoZPR",
        "outputId": "f09933de-72e5-4053-9df7-b444129b25a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs:\n",
            "torch.Size([4, 10])\n",
            "tensor([[ 1, 49, 54, 65,  1, 37, 35, 41, 43, 48],\n",
            "        [39, 52, 35,  7,  1, 49, 51, 43, 35,  9],\n",
            "        [45, 61,  1, 37, 48, 47, 53, 51, 35,  1],\n",
            "        [ 1, 35,  1, 53, 35, 47, 53, 35,  1, 49]])\n",
            "targets:\n",
            "torch.Size([4, 10])\n",
            "tensor([[49, 54, 65,  1, 37, 35, 41, 43, 48, 47],\n",
            "        [52, 35,  7,  1, 49, 51, 43, 35,  9,  0],\n",
            "        [61,  1, 37, 48, 47, 53, 51, 35,  1,  4],\n",
            "        [35,  1, 53, 35, 47, 53, 35,  1, 49, 35]])\n",
            "----\n",
            "when input is [1] the target: 49\n",
            "when input is [1, 49] the target: 54\n",
            "when input is [1, 49, 54] the target: 65\n",
            "when input is [1, 49, 54, 65] the target: 1\n",
            "when input is [1, 49, 54, 65, 1] the target: 37\n",
            "when input is [1, 49, 54, 65, 1, 37] the target: 35\n",
            "when input is [1, 49, 54, 65, 1, 37, 35] the target: 41\n",
            "when input is [1, 49, 54, 65, 1, 37, 35, 41] the target: 43\n",
            "when input is [1, 49, 54, 65, 1, 37, 35, 41, 43] the target: 48\n",
            "when input is [1, 49, 54, 65, 1, 37, 35, 41, 43, 48] the target: 47\n",
            "when input is [39] the target: 52\n",
            "when input is [39, 52] the target: 35\n",
            "when input is [39, 52, 35] the target: 7\n",
            "when input is [39, 52, 35, 7] the target: 1\n",
            "when input is [39, 52, 35, 7, 1] the target: 49\n",
            "when input is [39, 52, 35, 7, 1, 49] the target: 51\n",
            "when input is [39, 52, 35, 7, 1, 49, 51] the target: 43\n",
            "when input is [39, 52, 35, 7, 1, 49, 51, 43] the target: 35\n",
            "when input is [39, 52, 35, 7, 1, 49, 51, 43, 35] the target: 9\n",
            "when input is [39, 52, 35, 7, 1, 49, 51, 43, 35, 9] the target: 0\n",
            "when input is [45] the target: 61\n",
            "when input is [45, 61] the target: 1\n",
            "when input is [45, 61, 1] the target: 37\n",
            "when input is [45, 61, 1, 37] the target: 48\n",
            "when input is [45, 61, 1, 37, 48] the target: 47\n",
            "when input is [45, 61, 1, 37, 48, 47] the target: 53\n",
            "when input is [45, 61, 1, 37, 48, 47, 53] the target: 51\n",
            "when input is [45, 61, 1, 37, 48, 47, 53, 51] the target: 35\n",
            "when input is [45, 61, 1, 37, 48, 47, 53, 51, 35] the target: 1\n",
            "when input is [45, 61, 1, 37, 48, 47, 53, 51, 35, 1] the target: 4\n",
            "when input is [1] the target: 35\n",
            "when input is [1, 35] the target: 1\n",
            "when input is [1, 35, 1] the target: 53\n",
            "when input is [1, 35, 1, 53] the target: 35\n",
            "when input is [1, 35, 1, 53, 35] the target: 47\n",
            "when input is [1, 35, 1, 53, 35, 47] the target: 53\n",
            "when input is [1, 35, 1, 53, 35, 47, 53] the target: 35\n",
            "when input is [1, 35, 1, 53, 35, 47, 53, 35] the target: 1\n",
            "when input is [1, 35, 1, 53, 35, 47, 53, 35, 1] the target: 49\n",
            "when input is [1, 35, 1, 53, 35, 47, 53, 35, 1, 49] the target: 35\n"
          ]
        }
      ],
      "source": [
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    return x, y\n",
        "\n",
        "xb, yb = get_batch('train')\n",
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('targets:')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "print('----')\n",
        "\n",
        "for b in range(batch_size): # batch dimension\n",
        "    for t in range(block_size): # time dimension\n",
        "        context = xb[b, :t+1]\n",
        "        target = yb[b,t]\n",
        "        print(f\"when input is {context.tolist()} the target: {target}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UeRmGcHQoZPR",
        "outputId": "e0caf5fd-b0ea-4f96-9351-5ef0c11d5e4e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BigramLanguageModel(\n",
              "  (token_embedding_table): Embedding(68, 68)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(42)\n",
        "\n",
        "class BigramLanguageModel(nn.Module): #this simply calculate the probability of next token only on the previous token\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C) #reshape\n",
        "            targets = targets.view(B*T) #reshape in a single vector\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "m = BigramLanguageModel(vocab_size)\n",
        "m"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "logits, loss = m(xb, yb)\n",
        "print(logits.shape)\n",
        "print(loss)\n",
        "\n",
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y5_UWYdPqkOL",
        "outputId": "b3d8fa02-5b00-41f3-da86-cf78b1c8d7c1"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([40, 68])\n",
            "tensor(4.5837, grad_fn=<NllLossBackward0>)\n",
            "\n",
            "\n",
            "N)QNAlBPdìl~NnézfGCf.ehAEèòz(BèOdBz(UòlN~McXtRFjZHD!ìxZbìRbqóïOdMè,x!òx!eZbU)~b yXTQ?GgP?-DnCDùSùz~\n",
            "CPU times: user 24.6 ms, sys: 3.78 ms, total: 28.4 ms\n",
            "Wall time: 33.4 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n",
        "\n",
        "for steps in range(20000): # increase number of steps for good results...\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = m(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(loss.item())"
      ],
      "metadata": {
        "id": "81ESLYXBHNWC",
        "outputId": "e32f850e-8b62-434e-d434-015daabbd61b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.2792601585388184\n",
            "CPU times: user 17.4 s, sys: 34.5 ms, total: 17.4 s\n",
            "Wall time: 17.7 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=400)[0].tolist()))"
      ],
      "metadata": {
        "id": "ivJyD7T4H2PN",
        "outputId": "29a7a4c1-6fd9-4d47-e2c3-afe57632817b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "i.\n",
            " li qucond'Orasé tra Al Ci\".\n",
            "meine a areaiza mavitenca.\n",
            "Norci Pon pa nere,\n",
            "(!~à ico;\n",
            " Aloloch!Or ggl'averada \".\n",
            "pro,\n",
            "\n",
            " ggia mmputoné ggrca canatie ci chiuevero quamal'aie  amediggundo linunoge le mmicchen pe ghé quninon'vrotronzzi o, l  'osaba llo iatrrbido  sui anfià cci cherntuò, a vargred'e Se mi chedesi,\n",
            "pr itomisppa ttridatonua\n",
            "di,\n",
            "ceranen ti tta r punatrinai nte cora  scocirgoccotuina gin\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}