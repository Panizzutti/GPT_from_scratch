{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#!git clone https://github.com/Panizzutti/GPT_from_scratch.git"
      ],
      "metadata": {
        "id": "OI4pMLxooyDs"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "0yoM_C2xoZPQ"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "CHojd65PoZPQ"
      },
      "outputs": [],
      "source": [
        "with open('divina_commedia.txt', 'r', encoding = 'utf-8') as f:\n",
        "    text = f.read()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "qmzHjKvjoZPQ",
        "outputId": "63d70a72-c099-4857-d597-d43182022246",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LA DIVINA COMMEDIA\n",
            "di Dante Alighieri\n",
            "INFERNO\n",
            "\n",
            "\n",
            "\n",
            "Inferno: Canto I\n",
            "\n",
            "  Nel mezzo del cammin di nostra vita\n",
            "mi ritrovai per una selva oscura\n",
            "ché la diritta via era smarrita.\n",
            "  Ahi quanto a dir qual era è\n"
          ]
        }
      ],
      "source": [
        "print(text[:200])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "1Zh-yVc0oZPQ",
        "outputId": "d987dfab-5345-47a7-c8f0-c7121a229c5b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "537093"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ],
      "source": [
        "len(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "kb0m9Z0QoZPR",
        "outputId": "74d84d6f-936e-43c5-c818-7d9546b47e0c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !\"'(),-.:;?ABCDEFGHILMNOPQRSTUVXZabcdefghijlmnopqrstuvxyz~àèéìïòóù\n",
            "68\n"
          ]
        }
      ],
      "source": [
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(\"\".join(chars))\n",
        "print(vocab_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "mEpIirazoZPR"
      },
      "outputs": [],
      "source": [
        "char_to_int = { ch: i for i,ch in enumerate(chars)}\n",
        "int_to_char = { i: ch for i,ch in enumerate(chars)}\n",
        "encode = lambda s: [char_to_int[c] for c in s] #encoder function\n",
        "decode = lambda l: \"\".join(int_to_char[i] for i in l) #decoder function\n",
        "\n",
        "#not byte pair encoding, for simplicity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "QYC9rT7XoZPR",
        "outputId": "952aeec7-6fb9-4b7c-ea48-3cbe588cc5f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cianoxxj'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 53
        }
      ],
      "source": [
        "decode(encode(\"cianoxxj\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "jW8AJNwCoZPR",
        "outputId": "d10b42d9-b8b9-4a43-a791-eb2cdf09b667",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[37, 43, 35, 48, 1, 46, 48, 47, 38, 48]"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ],
      "source": [
        "encode(\"ciao mondo\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "yqELeRRCoZPR"
      },
      "outputs": [],
      "source": [
        "data = torch.tensor(encode(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "EkaneMjCoZPR"
      },
      "outputs": [],
      "source": [
        "n = int(0.9 * len(data))\n",
        "train_data= data[:n]\n",
        "val_data= data[n:]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "ziigIi8doZPR",
        "outputId": "e92d7ebe-30d9-4be2-fa45-56730718c41c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([22, 13,  1, 16, 21, 32, 21, 24, 13,  1, 15])"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ],
      "source": [
        "batch_size = 4\n",
        "block_size= 10\n",
        "\n",
        "train_data[:block_size+1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "SllBaAyNoZPR",
        "outputId": "5f773c43-868b-49e0-e9c6-d1106ef8b1b0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs:\n",
            "torch.Size([4, 10])\n",
            "tensor([[43,  1, 55, 48, 52, 53, 51, 43,  1, 52],\n",
            "        [ 9,  0,  1,  1, 17,  1, 43, 48,  7,  1],\n",
            "        [ 1, 49, 51, 39, 37, 48, 11,  0, 39,  1],\n",
            "        [48, 43,  0, 35, 45, 45, 48, 51, 35,  1]])\n",
            "targets:\n",
            "torch.Size([4, 10])\n",
            "tensor([[ 1, 55, 48, 52, 53, 51, 43,  1, 52, 37],\n",
            "        [ 0,  1,  1, 17,  1, 43, 48,  7,  1, 37],\n",
            "        [49, 51, 39, 37, 48, 11,  0, 39,  1, 52],\n",
            "        [43,  0, 35, 45, 45, 48, 51, 35,  1, 37]])\n",
            "----\n",
            "when input is [43] the target: 1\n",
            "when input is [43, 1] the target: 55\n",
            "when input is [43, 1, 55] the target: 48\n",
            "when input is [43, 1, 55, 48] the target: 52\n",
            "when input is [43, 1, 55, 48, 52] the target: 53\n",
            "when input is [43, 1, 55, 48, 52, 53] the target: 51\n",
            "when input is [43, 1, 55, 48, 52, 53, 51] the target: 43\n",
            "when input is [43, 1, 55, 48, 52, 53, 51, 43] the target: 1\n",
            "when input is [43, 1, 55, 48, 52, 53, 51, 43, 1] the target: 52\n",
            "when input is [43, 1, 55, 48, 52, 53, 51, 43, 1, 52] the target: 37\n",
            "when input is [9] the target: 0\n",
            "when input is [9, 0] the target: 1\n",
            "when input is [9, 0, 1] the target: 1\n",
            "when input is [9, 0, 1, 1] the target: 17\n",
            "when input is [9, 0, 1, 1, 17] the target: 1\n",
            "when input is [9, 0, 1, 1, 17, 1] the target: 43\n",
            "when input is [9, 0, 1, 1, 17, 1, 43] the target: 48\n",
            "when input is [9, 0, 1, 1, 17, 1, 43, 48] the target: 7\n",
            "when input is [9, 0, 1, 1, 17, 1, 43, 48, 7] the target: 1\n",
            "when input is [9, 0, 1, 1, 17, 1, 43, 48, 7, 1] the target: 37\n",
            "when input is [1] the target: 49\n",
            "when input is [1, 49] the target: 51\n",
            "when input is [1, 49, 51] the target: 39\n",
            "when input is [1, 49, 51, 39] the target: 37\n",
            "when input is [1, 49, 51, 39, 37] the target: 48\n",
            "when input is [1, 49, 51, 39, 37, 48] the target: 11\n",
            "when input is [1, 49, 51, 39, 37, 48, 11] the target: 0\n",
            "when input is [1, 49, 51, 39, 37, 48, 11, 0] the target: 39\n",
            "when input is [1, 49, 51, 39, 37, 48, 11, 0, 39] the target: 1\n",
            "when input is [1, 49, 51, 39, 37, 48, 11, 0, 39, 1] the target: 52\n",
            "when input is [48] the target: 43\n",
            "when input is [48, 43] the target: 0\n",
            "when input is [48, 43, 0] the target: 35\n",
            "when input is [48, 43, 0, 35] the target: 45\n",
            "when input is [48, 43, 0, 35, 45] the target: 45\n",
            "when input is [48, 43, 0, 35, 45, 45] the target: 48\n",
            "when input is [48, 43, 0, 35, 45, 45, 48] the target: 51\n",
            "when input is [48, 43, 0, 35, 45, 45, 48, 51] the target: 35\n",
            "when input is [48, 43, 0, 35, 45, 45, 48, 51, 35] the target: 1\n",
            "when input is [48, 43, 0, 35, 45, 45, 48, 51, 35, 1] the target: 37\n"
          ]
        }
      ],
      "source": [
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    return x, y\n",
        "\n",
        "xb, yb = get_batch('train')\n",
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('targets:')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "print('----')\n",
        "\n",
        "for b in range(batch_size): # batch dimension\n",
        "    for t in range(block_size): # time dimension\n",
        "        context = xb[b, :t+1]\n",
        "        target = yb[b,t]\n",
        "        print(f\"when input is {context.tolist()} the target: {target}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "UeRmGcHQoZPR",
        "outputId": "178cb622-beec-4fcd-8661-e66d8b143383",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BigramLanguageModel(\n",
              "  (token_embedding_table): Embedding(68, 68)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(42)\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "m = BigramLanguageModel(vocab_size)\n",
        "m"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logits, loss = m(xb, yb)\n",
        "print(logits.shape)\n",
        "print(loss)\n",
        "\n",
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
      ],
      "metadata": {
        "id": "y5_UWYdPqkOL",
        "outputId": "03d86d6d-10a7-41d8-f524-9fba7774260d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([40, 68])\n",
            "tensor(4.3539, grad_fn=<NllLossBackward0>)\n",
            "\n",
            "\n",
            "N)QNAlBPdìl~NnézfGCf.ehAEèòz(BèOdBz(UòlN~McXtRFjZHD!ìxZbìRbqóïOdMè,x!òx!eZbU)~b yXTQ?GgP?-DnCDùSùz~\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}